{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import json\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "554e29fe5f5c22dfc5e1627fb2d65fc836182bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Dataset ... \n",
      "Prepare text data of Train and Test ... \n",
      "TF-IDF on text data ... \n"
     ]
    }
   ],
   "source": [
    "print (\"Read Dataset ... \")\n",
    "def read_dataset(path):\n",
    "\treturn json.load(open(path)) \n",
    "train = read_dataset('train.json')\n",
    "test = read_dataset('test.json')\n",
    "\n",
    "# Text Data Features\n",
    "print (\"Prepare text data of Train and Test ... \")\n",
    "def generate_text(data):\n",
    "\ttext_data = [\" \".join(doc['ingredients']).lower() for doc in data]\n",
    "\treturn text_data \n",
    "\n",
    "train_text = generate_text(train)\n",
    "test_text = generate_text(test)\n",
    "target = [doc['cuisine'] for doc in train]\n",
    "\n",
    "# Feature Engineering \n",
    "print (\"TF-IDF on text data ... \")\n",
    "tfidf = TfidfVectorizer(binary=True, norm='l2')\n",
    "def tfidf_features(txt, flag):\n",
    "    if flag == \"train\":\n",
    "    \tx = tfidf.fit_transform(txt)\n",
    "    else:\n",
    "\t    x = tfidf.transform(txt)\n",
    "    x = x.astype('float32')\n",
    "    return x \n",
    "\n",
    "X = tfidf_features(train_text, flag=\"train\")\n",
    "X_test = tfidf_features(test_text, flag=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "39991253a5327fbbf2421cabb7302579b32c648a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encode the Target Variable ... \n"
     ]
    }
   ],
   "source": [
    "# Label Encoding - Target \n",
    "print (\"Label Encode the Target Variable ... \")\n",
    "lb = LabelEncoder()\n",
    "y = lb.fit_transform(target)\n",
    "# X_scaled = preprocessing.scale(X.toarray())\n",
    "# test_scaled = preprocessing.scale(X_test.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "13f5c9f450aa7c0131f0a8a2fec5924387c5f252"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=0.95)\n",
    "# X_pca = pca.fit_transform(X.toarray())\n",
    "# print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "da57208d5ac962b6e51759bbc1af31a294e22d4d"
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y , random_state = 0,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7930861093651791\n",
      "Wall time: 12min 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMClassifier\n",
    "m1 = LGBMClassifier(n_estimators=3000, max_depth=15)\n",
    "m1.fit(x_train, y_train)\n",
    "print(m1.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.svm import NuSVC\n",
    "m2 = SVC(C=300, # penalty parameter, setting it to a larger value \n",
    "                 kernel='rbf', # kernel type, rbf working fine here\n",
    "                 degree=3, # default value, not tuned yet\n",
    "                 gamma=2, # kernel coefficient, not tuned yet\n",
    "                 coef0=1, # change to 1 from default value of 0.0\n",
    "                 shrinking=True, # using shrinking heuristics\n",
    "                 tol=0.000001, # stopping criterion tolerance \n",
    "                 probability=False, # no need to enable probability estimates\n",
    "                 class_weight=None, # all classes are treated equally \n",
    "                 verbose=0, # print the logs \n",
    "                 max_iter=-1, # no limit, let it run\n",
    "                 decision_function_shape=None, # will use one vs rest explicitly \n",
    "                 random_state=None)\n",
    "m2.fit(x_train,y_train)\n",
    "print(m2.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "d44e3877539e80046380b639eb7e97460dc531d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the model ... \n",
      "0.7859208045254557\n",
      "Wall time: 39min 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print (\"Train the model ... \")\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model = XGBClassifier( learning_rate =0.1,\n",
    "                         n_estimators=1000,\n",
    "                         max_depth=25,\n",
    "                         min_child_weight=1,\n",
    "                         gamma=0,\n",
    "                         subsample=0.8,\n",
    "                         colsample_bytree=0.8,\n",
    "                         objective= 'multi:softmax',\n",
    "                         nthread=4,\n",
    "                         tree_method ='hist',\n",
    "                         scale_pos_weight=1,\n",
    "                         seed=27)\n",
    "# model = SVC(C=100, # penalty parameter, setting it to a larger value \n",
    "#                  kernel='rbf', # kernel type, rbf working fine here\n",
    "#                  degree=3, # default value, not tuned yet\n",
    "#                  gamma=2, # kernel coefficient, not tuned yet\n",
    "#                  coef0=1, # change to 1 from default value of 0.0\n",
    "#                  shrinking=True, # using shrinking heuristics\n",
    "#                  tol=0.00001, # stopping criterion tolerance \n",
    "#                  probability=False, # no need to enable probability estimates\n",
    "#                  class_weight=None, # all classes are treated equally \n",
    "#                  verbose=1, # print the logs \n",
    "#                  max_iter=-1, # no limit, let it run\n",
    "#                  decision_function_shape=None, # will use one vs rest explicitly \n",
    "#                  random_state=None)\n",
    "\n",
    "# model=VotingClassifier(estimators=[('clf1',LogisticRegression(C=10,dual=False)),\n",
    "#                                   ('clf2',SVC(C=100,gamma=2,kernel='rbf',probability=True)),\n",
    "#                                   ('clf',RandomForestClassifier(n_jobs=-1, n_estimators=100,min_samples_leaf=3,max_features=0.7, oob_score=True))],voting='soft',weights=[1,1,1])\n",
    "\n",
    "# model = OneVsRestClassifier(classifier, n_jobs=-1)\n",
    "# model = OneVsOneClassifier(classifier, n_jobs=4)\n",
    "model.fit(x_train, y_train)\n",
    "print(model.score(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "## DNN Solution\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, GlobalAveragePooling1D, BatchNormalization, ActivityRegularization\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.cross_validation import train_test_split\n",
    "Y = to_categorical(y, num_classes=20)\n",
    "random_seed = 2\n",
    "# Split the train and the validation set for the fitting\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X.toarray(), Y, test_size = 0.2, random_state=random_seed)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='tanh', input_shape=(3051,)))\n",
    "# model.add(ActivityRegularization(l1=0.0, l2=0.2))\n",
    "model.add(Dense(256, activation = \"tanh\"))\n",
    "model.add(Dense(128, activation = \"tanh\"))\n",
    "# model.add(ActivityRegularization(l1=0.0, l2=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation = \"tanh\"))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(20, activation = \"softmax\"))\n",
    "\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=1e-04)\n",
    "optimizer1 = Adam(lr=0.001, decay=1e-4)\n",
    "optimizer2 = SGD(lr=0.1, decay=1e-4)\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=0.01, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.0001)\n",
    "earlystopping = EarlyStopping(monitor='val_acc', patience=0.001, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31819 samples, validate on 7955 samples\n",
      "Epoch 1/200\n",
      "31819/31819 [==============================] - 1s 26us/step - loss: 0.9334 - acc: 0.7551 - val_loss: 1.1942 - val_acc: 0.7032\n",
      "Epoch 2/200\n",
      "31819/31819 [==============================] - 1s 25us/step - loss: 0.9322 - acc: 0.7550 - val_loss: 1.1942 - val_acc: 0.7023\n",
      "Epoch 3/200\n",
      "31819/31819 [==============================] - 1s 27us/step - loss: 0.9266 - acc: 0.7547 - val_loss: 1.1889 - val_acc: 0.7077\n",
      "Epoch 4/200\n",
      "31819/31819 [==============================] - 1s 25us/step - loss: 0.9222 - acc: 0.7560 - val_loss: 1.1888 - val_acc: 0.7060\n",
      "Epoch 5/200\n",
      "31819/31819 [==============================] - 1s 27us/step - loss: 0.9202 - acc: 0.7569 - val_loss: 1.1890 - val_acc: 0.7048\n",
      "Epoch 6/200\n",
      "31819/31819 [==============================] - 1s 24us/step - loss: 0.9218 - acc: 0.7558 - val_loss: 1.1814 - val_acc: 0.7045\n",
      "Epoch 7/200\n",
      "31819/31819 [==============================] - 1s 27us/step - loss: 0.9208 - acc: 0.7545 - val_loss: 1.1820 - val_acc: 0.7071\n",
      "Epoch 8/200\n",
      "31819/31819 [==============================] - 1s 26us/step - loss: 0.9202 - acc: 0.7582 - val_loss: 1.1918 - val_acc: 0.7043\n",
      "Epoch 9/200\n",
      "31819/31819 [==============================] - 1s 25us/step - loss: 0.9103 - acc: 0.7590 - val_loss: 1.2101 - val_acc: 0.7001\n",
      "Epoch 10/200\n",
      "31819/31819 [==============================] - 1s 24us/step - loss: 0.9153 - acc: 0.7583 - val_loss: 1.1903 - val_acc: 0.7055\n",
      "Epoch 11/200\n",
      "31819/31819 [==============================] - 1s 24us/step - loss: 0.9111 - acc: 0.7597 - val_loss: 1.1857 - val_acc: 0.7070\n",
      "Epoch 12/200\n",
      "31819/31819 [==============================] - 1s 27us/step - loss: 0.9076 - acc: 0.7576 - val_loss: 1.1925 - val_acc: 0.7051\n",
      "Epoch 13/200\n",
      "31819/31819 [==============================] - 1s 25us/step - loss: 0.9053 - acc: 0.7601 - val_loss: 1.1926 - val_acc: 0.7017\n",
      "Epoch 14/200\n",
      "31819/31819 [==============================] - 1s 24us/step - loss: 0.9067 - acc: 0.7589 - val_loss: 1.1764 - val_acc: 0.7056\n",
      "Epoch 15/200\n",
      "  512/31819 [..............................] - ETA: 0s - loss: 1.0004 - acc: 0.7188"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-b712bb4f1e4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                       verbose=1,callbacks=[learning_rate_reduction])\n\u001b[0m",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                      epochs=200,\n",
    "                      batch_size=512,\n",
    "                      validation_data=(X_val,Y_val),\n",
    "                      verbose=1,callbacks=[learning_rate_reduction])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
