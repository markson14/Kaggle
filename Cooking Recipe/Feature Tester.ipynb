{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import json\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "554e29fe5f5c22dfc5e1627fb2d65fc836182bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Dataset ... \n",
      "Prepare text data of Train and Test ... \n",
      "TF-IDF on text data ... \n"
     ]
    }
   ],
   "source": [
    "print (\"Read Dataset ... \")\n",
    "def read_dataset(path):\n",
    "\treturn json.load(open(path)) \n",
    "train = read_dataset('train.json')\n",
    "# print(len(train))\n",
    "test = read_dataset('test.json')\n",
    "train1 = read_dataset('cooking_train.json')\n",
    "\n",
    "# Text Data Features\n",
    "print (\"Prepare text data of Train and Test ... \")\n",
    "def generate_text(data):\n",
    "\ttext_data = [\" \".join(doc['ingredients']).lower() for doc in data]\n",
    "\treturn text_data \n",
    "\n",
    "train_text = generate_text(train)\n",
    "train1_text = generate_text(train1)\n",
    "test_text = generate_text(test)\n",
    "target1 = [doc['cuisine'] for doc in train]\n",
    "target2 = [doc['cuisine'] for doc in train1]\n",
    "\n",
    "for i in range(len(train_text)-1):\n",
    "    train_text[i] = train_text[i].replace('salt','')\n",
    "\n",
    "# Feature Engineering \n",
    "print (\"TF-IDF on text data ... \")\n",
    "tfidf = TfidfVectorizer(binary=True, norm='l2')\n",
    "# tfidf = CountVectorizer(binary=True)\n",
    "def tfidf_features(txt, flag):\n",
    "    if flag == \"train\":\n",
    "    \tx = tfidf.fit_transform(txt)\n",
    "    else:\n",
    "\t    x = tfidf.transform(txt)\n",
    "    x = x.astype('float')\n",
    "    return x \n",
    "\n",
    "X = tfidf_features(train_text, flag=\"train\")\n",
    "X_test = tfidf_features(test_text, flag=\"test\")\n",
    "XX = tfidf_features(train1_text, flag=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(X.toarray())\n",
    "b = pd.DataFrame(XX.toarray())\n",
    "c = a.append(b[:])\n",
    "\n",
    "X_test = pd.DataFrame(X_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target1 + target2[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69774\n",
      "69774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9944"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(target))\n",
    "print(len(c))\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "39991253a5327fbbf2421cabb7302579b32c648a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encode the Target Variable ... \n"
     ]
    }
   ],
   "source": [
    "# Label Encoding - Target \n",
    "print (\"Label Encode the Target Variable ... \")\n",
    "lb = LabelEncoder()\n",
    "y = lb.fit_transform(target)\n",
    "# X_scaled = preprocessing.scale(X.toarray())\n",
    "# test_scaled = preprocessing.scale(X_test.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "13f5c9f450aa7c0131f0a8a2fec5924387c5f252"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=0.95)\n",
    "# X_pca = pca.fit_transform(X.toarray())\n",
    "# print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "da57208d5ac962b6e51759bbc1af31a294e22d4d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(c, y , random_state = 0,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = SVC(C=100, # penalty parameter, setting it to a larger value \n",
    "                 kernel='rbf', # kernel type, rbf working fine here\n",
    "                 degree=3, # default value, not tuned yet\n",
    "                 gamma=2, # kernel coefficient, not tuned yet\n",
    "                 coef0=1, # change to 1 from default value of 0.0\n",
    "                 shrinking=True, # using shrinking heuristics\n",
    "                 tol=0.00001, # stopping criterion tolerance \n",
    "                 probability=False, # no need to enable probability estimates\n",
    "                 class_weight=None, # all classes are treated equally \n",
    "                 verbose=1, # print the logs \n",
    "                 max_iter=-1, # no limit, let it run\n",
    "                 decision_function_shape=None, # will use one vs rest explicitly \n",
    "                 random_state=None)\n",
    "\n",
    "# model = BaggingClassifier(m2, max_samples=0.5, max_features=0.5)\n",
    "# model = AdaBoostClassifier(base_estimator=m2,algorithm='SAMME',learning_rate=0.1)\n",
    "# model = OneVsRestClassifier(classifier, n_jobs=-1)\n",
    "\n",
    "model.fit(c,y)\n",
    "preds['svm'] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "m1 = LGBMClassifier(n_estimators=550, max_depth=15)\n",
    "\n",
    "# model = BaggingClassifier(m1, max_samples=0.8, max_features=0.8)\n",
    "m1.fit(c[:50000],y[:50000])\n",
    "y_pred = m1.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "preds['lgbm'] = lb.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "op1 = pd.read_csv('svm_output (1).csv')\n",
    "op2 = pd.read_csv('svm_output_ensemble.csv')\n",
    "preds['svm1'] = op1.cuisine\n",
    "preds['svm2'] = op2.cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnn</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>svm1</th>\n",
       "      <th>svm2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>british</td>\n",
       "      <td>irish</td>\n",
       "      <td>irish</td>\n",
       "      <td>irish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cajun_creole</td>\n",
       "      <td>cajun_creole</td>\n",
       "      <td>cajun_creole</td>\n",
       "      <td>cajun_creole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>french</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>greek</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chinese</td>\n",
       "      <td>chinese</td>\n",
       "      <td>chinese</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>british</td>\n",
       "      <td>british</td>\n",
       "      <td>irish</td>\n",
       "      <td>british</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>greek</td>\n",
       "      <td>greek</td>\n",
       "      <td>greek</td>\n",
       "      <td>greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>chinese</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>british</td>\n",
       "      <td>british</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>italian</td>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>spanish</td>\n",
       "      <td>french</td>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>japanese</td>\n",
       "      <td>japanese</td>\n",
       "      <td>japanese</td>\n",
       "      <td>japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "      <td>irish</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vietnamese</td>\n",
       "      <td>vietnamese</td>\n",
       "      <td>vietnamese</td>\n",
       "      <td>vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>thai</td>\n",
       "      <td>thai</td>\n",
       "      <td>thai</td>\n",
       "      <td>thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>korean</td>\n",
       "      <td>chinese</td>\n",
       "      <td>korean</td>\n",
       "      <td>korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mexican</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9914</th>\n",
       "      <td>thai</td>\n",
       "      <td>vietnamese</td>\n",
       "      <td>vietnamese</td>\n",
       "      <td>vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915</th>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>french</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>greek</td>\n",
       "      <td>greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917</th>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9919</th>\n",
       "      <td>filipino</td>\n",
       "      <td>mexican</td>\n",
       "      <td>jamaican</td>\n",
       "      <td>brazilian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9920</th>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9921</th>\n",
       "      <td>thai</td>\n",
       "      <td>thai</td>\n",
       "      <td>thai</td>\n",
       "      <td>thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9922</th>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9923</th>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9924</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9925</th>\n",
       "      <td>british</td>\n",
       "      <td>british</td>\n",
       "      <td>british</td>\n",
       "      <td>british</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9926</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9927</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9928</th>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9929</th>\n",
       "      <td>cajun_creole</td>\n",
       "      <td>cajun_creole</td>\n",
       "      <td>cajun_creole</td>\n",
       "      <td>cajun_creole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9930</th>\n",
       "      <td>thai</td>\n",
       "      <td>thai</td>\n",
       "      <td>chinese</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9931</th>\n",
       "      <td>brazilian</td>\n",
       "      <td>spanish</td>\n",
       "      <td>spanish</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9932</th>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9933</th>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9934</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9935</th>\n",
       "      <td>chinese</td>\n",
       "      <td>chinese</td>\n",
       "      <td>chinese</td>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9936</th>\n",
       "      <td>italian</td>\n",
       "      <td>french</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9937</th>\n",
       "      <td>thai</td>\n",
       "      <td>thai</td>\n",
       "      <td>thai</td>\n",
       "      <td>thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9938</th>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "      <td>indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9939</th>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9940</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9941</th>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9942</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9943</th>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "      <td>mexican</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9944 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               cnn          lgbm          svm1          svm2\n",
       "0          british         irish         irish         irish\n",
       "1      southern_us   southern_us   southern_us   southern_us\n",
       "2          italian       italian       italian       italian\n",
       "3     cajun_creole  cajun_creole  cajun_creole  cajun_creole\n",
       "4          italian       italian       italian       italian\n",
       "5      southern_us   southern_us   southern_us   southern_us\n",
       "6           french   southern_us         greek       italian\n",
       "7          chinese       chinese       chinese       chinese\n",
       "8          mexican       mexican       mexican       mexican\n",
       "9          british       british         irish       british\n",
       "10         italian       italian       italian       italian\n",
       "11           greek         greek         greek         greek\n",
       "12          indian        indian        indian        indian\n",
       "13         italian       italian       italian       italian\n",
       "14         chinese   southern_us       british       british\n",
       "15         italian        french        french        french\n",
       "16         spanish        french       mexican       mexican\n",
       "17     southern_us   southern_us   southern_us   southern_us\n",
       "18         mexican       mexican       mexican       mexican\n",
       "19     southern_us   southern_us   southern_us   southern_us\n",
       "20        japanese      japanese      japanese      japanese\n",
       "21          indian        indian        indian        indian\n",
       "22          french        french         irish        french\n",
       "23      vietnamese    vietnamese    vietnamese    vietnamese\n",
       "24         italian       italian       italian       italian\n",
       "25     southern_us   southern_us   southern_us   southern_us\n",
       "26            thai          thai          thai          thai\n",
       "27          korean       chinese        korean        korean\n",
       "28         italian       italian       italian       italian\n",
       "29         mexican   southern_us   southern_us   southern_us\n",
       "...            ...           ...           ...           ...\n",
       "9914          thai    vietnamese    vietnamese    vietnamese\n",
       "9915       italian       italian       italian       italian\n",
       "9916        french   southern_us         greek         greek\n",
       "9917       italian       italian       italian       italian\n",
       "9918   southern_us   southern_us   southern_us   southern_us\n",
       "9919      filipino       mexican      jamaican     brazilian\n",
       "9920       italian       italian       italian       italian\n",
       "9921          thai          thai          thai          thai\n",
       "9922        indian        indian        indian        indian\n",
       "9923       italian       italian       italian       italian\n",
       "9924   southern_us   southern_us   southern_us   southern_us\n",
       "9925       british       british       british       british\n",
       "9926   southern_us   southern_us   southern_us   southern_us\n",
       "9927   southern_us   southern_us   southern_us   southern_us\n",
       "9928        indian        indian        indian        indian\n",
       "9929  cajun_creole  cajun_creole  cajun_creole  cajun_creole\n",
       "9930          thai          thai       chinese       chinese\n",
       "9931     brazilian       spanish       spanish       spanish\n",
       "9932       mexican       mexican       mexican       mexican\n",
       "9933       mexican       mexican       mexican       mexican\n",
       "9934   southern_us   southern_us   southern_us   southern_us\n",
       "9935       chinese       chinese       chinese       chinese\n",
       "9936       italian        french       italian       italian\n",
       "9937          thai          thai          thai          thai\n",
       "9938        indian        indian        indian        indian\n",
       "9939        french        french        french        french\n",
       "9940   southern_us   southern_us   southern_us   southern_us\n",
       "9941       italian       italian       italian       italian\n",
       "9942   southern_us   southern_us   southern_us   southern_us\n",
       "9943       mexican       mexican       mexican       mexican\n",
       "\n",
       "[9944 rows x 4 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(preds)\n",
    "cols.insert(0,cols.pop(cols.index('svm2')))\n",
    "preds12 = preds.loc[:,cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds12 = preds12.mode(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "## DNN Solution\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, GlobalAveragePooling1D, BatchNormalization, ActivityRegularization\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.cross_validation import train_test_split\n",
    "Y = to_categorical(y, num_classes=20)\n",
    "random_seed = 2\n",
    "# Split the train and the validation set for the fitting\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(a, Y[:39774], test_size = 0.4, random_state=random_seed)\n",
    "\n",
    "X_train = X_train.append(b)\n",
    "yyy = list(Y_train)\n",
    "yyy.extend(list(Y[39774:]))\n",
    "Y_train = np.array(yyy)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(3051,)))\n",
    "# model.add(ActivityRegularization(l1=0.0, l2=0.2))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "# model.add(ActivityRegularization(l1=0.0, l2=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(20, activation = \"softmax\"))\n",
    "\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=1e-04)\n",
    "optimizer1 = Adam(lr=0.001, decay=1e-4)\n",
    "optimizer2 = SGD(lr=0.1, decay=1e-4)\n",
    "model.compile(optimizer = optimizer1 , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=0.01, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.0001)\n",
    "earlystopping = EarlyStopping(monitor='val_acc', patience=0.001, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53864 samples, validate on 15910 samples\n",
      "Epoch 1/35\n",
      "53864/53864 [==============================] - 4s 81us/step - loss: 1.6116 - acc: 0.5305 - val_loss: 1.0017 - val_acc: 0.7004\n",
      "Epoch 2/35\n",
      "53864/53864 [==============================] - 4s 75us/step - loss: 1.0136 - acc: 0.7121 - val_loss: 0.8367 - val_acc: 0.7626\n",
      "Epoch 3/35\n",
      "53864/53864 [==============================] - 4s 75us/step - loss: 0.7890 - acc: 0.7762 - val_loss: 0.7064 - val_acc: 0.8077\n",
      "Epoch 4/35\n",
      "53864/53864 [==============================] - 4s 73us/step - loss: 0.6299 - acc: 0.8195 - val_loss: 0.6288 - val_acc: 0.8422\n",
      "Epoch 5/35\n",
      "53864/53864 [==============================] - 4s 74us/step - loss: 0.4946 - acc: 0.8585 - val_loss: 0.5941 - val_acc: 0.8632\n",
      "Epoch 6/35\n",
      "53864/53864 [==============================] - 4s 73us/step - loss: 0.3973 - acc: 0.8874 - val_loss: 0.6035 - val_acc: 0.8723\n",
      "Epoch 7/35\n",
      "53864/53864 [==============================] - 4s 74us/step - loss: 0.3279 - acc: 0.9060 - val_loss: 0.5669 - val_acc: 0.8903\n",
      "Epoch 8/35\n",
      "53864/53864 [==============================] - 4s 78us/step - loss: 0.2774 - acc: 0.9196 - val_loss: 0.5588 - val_acc: 0.8947\n",
      "Epoch 9/35\n",
      "53864/53864 [==============================] - 4s 73us/step - loss: 0.2381 - acc: 0.9327 - val_loss: 0.5988 - val_acc: 0.9030\n",
      "Epoch 10/35\n",
      "53864/53864 [==============================] - 4s 76us/step - loss: 0.2042 - acc: 0.9399 - val_loss: 0.5956 - val_acc: 0.9114\n",
      "Epoch 11/35\n",
      "53864/53864 [==============================] - 4s 73us/step - loss: 0.1847 - acc: 0.9458 - val_loss: 0.5968 - val_acc: 0.9143\n",
      "Epoch 12/35\n",
      "53864/53864 [==============================] - 4s 75us/step - loss: 0.1662 - acc: 0.9504 - val_loss: 0.6078 - val_acc: 0.9161\n",
      "Epoch 13/35\n",
      "53864/53864 [==============================] - 4s 74us/step - loss: 0.1468 - acc: 0.9557 - val_loss: 0.6322 - val_acc: 0.9192\n",
      "Epoch 14/35\n",
      "53864/53864 [==============================] - 4s 73us/step - loss: 0.1396 - acc: 0.9593 - val_loss: 0.6605 - val_acc: 0.9203\n",
      "Epoch 15/35\n",
      "53864/53864 [==============================] - 4s 73us/step - loss: 0.1368 - acc: 0.9596 - val_loss: 0.6390 - val_acc: 0.9229\n",
      "Epoch 16/35\n",
      "53864/53864 [==============================] - 4s 73us/step - loss: 0.1198 - acc: 0.9640 - val_loss: 0.6863 - val_acc: 0.9244\n",
      "Epoch 17/35\n",
      "53864/53864 [==============================] - 4s 78us/step - loss: 0.1197 - acc: 0.9646 - val_loss: 0.6788 - val_acc: 0.9238\n",
      "Epoch 18/35\n",
      "53864/53864 [==============================] - 4s 76us/step - loss: 0.1123 - acc: 0.9670 - val_loss: 0.6916 - val_acc: 0.9274\n",
      "Epoch 19/35\n",
      "53864/53864 [==============================] - 4s 74us/step - loss: 0.1007 - acc: 0.9695 - val_loss: 0.6847 - val_acc: 0.9266\n",
      "Epoch 20/35\n",
      "53864/53864 [==============================] - 4s 73us/step - loss: 0.0958 - acc: 0.9703 - val_loss: 0.7070 - val_acc: 0.9280\n",
      "Epoch 21/35\n",
      "53864/53864 [==============================] - 4s 78us/step - loss: 0.0967 - acc: 0.9712 - val_loss: 0.6804 - val_acc: 0.9302\n",
      "Epoch 22/35\n",
      "53864/53864 [==============================] - 5s 84us/step - loss: 0.0900 - acc: 0.9726 - val_loss: 0.7334 - val_acc: 0.9305\n",
      "Epoch 23/35\n",
      "53864/53864 [==============================] - 4s 77us/step - loss: 0.0845 - acc: 0.9737 - val_loss: 0.7253 - val_acc: 0.9304\n",
      "Epoch 24/35\n",
      "53864/53864 [==============================] - 4s 77us/step - loss: 0.0863 - acc: 0.9735 - val_loss: 0.7224 - val_acc: 0.9307\n",
      "Epoch 25/35\n",
      "53864/53864 [==============================] - 4s 79us/step - loss: 0.0811 - acc: 0.9742 - val_loss: 0.7473 - val_acc: 0.9302\n",
      "Epoch 26/35\n",
      "53864/53864 [==============================] - 4s 77us/step - loss: 0.0815 - acc: 0.9751 - val_loss: 0.7326 - val_acc: 0.9322\n",
      "Epoch 27/35\n",
      "53864/53864 [==============================] - 4s 79us/step - loss: 0.0774 - acc: 0.9765 - val_loss: 0.7219 - val_acc: 0.9321\n",
      "Epoch 28/35\n",
      "53864/53864 [==============================] - 4s 77us/step - loss: 0.0738 - acc: 0.9765 - val_loss: 0.7374 - val_acc: 0.9332\n",
      "Epoch 29/35\n",
      "53864/53864 [==============================] - 4s 78us/step - loss: 0.0714 - acc: 0.9782 - val_loss: 0.7467 - val_acc: 0.9347\n",
      "Epoch 30/35\n",
      "53864/53864 [==============================] - 4s 78us/step - loss: 0.0731 - acc: 0.9771 - val_loss: 0.7594 - val_acc: 0.9326\n",
      "Epoch 31/35\n",
      "53864/53864 [==============================] - 4s 79us/step - loss: 0.0713 - acc: 0.9771 - val_loss: 0.7618 - val_acc: 0.9338\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 32/35\n",
      "53864/53864 [==============================] - 4s 79us/step - loss: 0.0621 - acc: 0.9802 - val_loss: 0.7618 - val_acc: 0.9362\n",
      "Epoch 33/35\n",
      "53864/53864 [==============================] - 4s 78us/step - loss: 0.0587 - acc: 0.9810 - val_loss: 0.7615 - val_acc: 0.9368\n",
      "Epoch 34/35\n",
      "53864/53864 [==============================] - 4s 80us/step - loss: 0.0565 - acc: 0.9803 - val_loss: 0.7817 - val_acc: 0.9366\n",
      "Epoch 35/35\n",
      "53864/53864 [==============================] - 4s 79us/step - loss: 0.0576 - acc: 0.9812 - val_loss: 0.7838 - val_acc: 0.9368\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                      epochs=30,\n",
    "                      batch_size=128,\n",
    "                      validation_data=(X_val,Y_val),\n",
    "                      verbose=1,callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "preds = pd.DataFrame()\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "results = np.argmax(y_pred,axis = 1)\n",
    "\n",
    "preds['cnn'] = lb.inverse_transform(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              irish\n",
       "1        southern_us\n",
       "2            italian\n",
       "3       cajun_creole\n",
       "4            italian\n",
       "5        southern_us\n",
       "6             french\n",
       "7            chinese\n",
       "8            mexican\n",
       "9            british\n",
       "10           italian\n",
       "11             greek\n",
       "12            indian\n",
       "13           italian\n",
       "14           british\n",
       "15            french\n",
       "16           mexican\n",
       "17       southern_us\n",
       "18           mexican\n",
       "19       southern_us\n",
       "20          japanese\n",
       "21            indian\n",
       "22            french\n",
       "23        vietnamese\n",
       "24           italian\n",
       "25       southern_us\n",
       "26              thai\n",
       "27            korean\n",
       "28           italian\n",
       "29       southern_us\n",
       "            ...     \n",
       "9914      vietnamese\n",
       "9915         italian\n",
       "9916           greek\n",
       "9917         italian\n",
       "9918     southern_us\n",
       "9919       brazilian\n",
       "9920         italian\n",
       "9921            thai\n",
       "9922          indian\n",
       "9923         italian\n",
       "9924     southern_us\n",
       "9925         british\n",
       "9926     southern_us\n",
       "9927     southern_us\n",
       "9928          indian\n",
       "9929    cajun_creole\n",
       "9930         chinese\n",
       "9931         spanish\n",
       "9932         mexican\n",
       "9933         mexican\n",
       "9934     southern_us\n",
       "9935         chinese\n",
       "9936         italian\n",
       "9937            thai\n",
       "9938          indian\n",
       "9939          french\n",
       "9940     southern_us\n",
       "9941         italian\n",
       "9942     southern_us\n",
       "9943         mexican\n",
       "Name: 0, Length: 9944, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds12[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Submission File ... \n"
     ]
    }
   ],
   "source": [
    "print (\"Generate Submission File ... \")\n",
    "test_id = [doc['id'] for doc in test]\n",
    "sub = pd.DataFrame({'id': test_id, 'cuisine': preds12[0]}, columns=['id', 'cuisine'])\n",
    "sub.to_csv('ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
