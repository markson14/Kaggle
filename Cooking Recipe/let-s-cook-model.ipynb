{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Let's cook model\n",
    "\n",
    "Let's combine what we've found so far.\n",
    "\n",
    "- [What are ingredients?](https://www.kaggle.com/rejasupotaro/what-are-ingredients) (Preprocessing & Feature extraction)\n",
    "- [Representations for ingredients](https://www.kaggle.com/rejasupotaro/representations-for-ingredients)\n",
    "\n",
    "Steps are below.\n",
    "\n",
    "1. Load dataset\n",
    "2. Remove outliers\n",
    "3. Preprocess\n",
    "4. Create model\n",
    "5. Check local CV\n",
    "6. Train model\n",
    "7. Check predicted values\n",
    "8. Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e71a483cf5522143e6accc7f89a246dd2b84eac"
   },
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "68cf27ffa9750f359ebeae70edf914a389697444"
   },
   "outputs": [],
   "source": [
    "train = pd.read_json('train.json')\n",
    "test = pd.read_json('test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa9226059c24efcaa31d78667aaaa3b9f1644cd8"
   },
   "source": [
    "## 2. Remove outliers\n",
    "\n",
    "I saw weird recipes in the dataset .\n",
    "\n",
    "- water => Japanese\n",
    "- butter => Indian\n",
    "- butter => French\n",
    "\n",
    "Let's filter such single-ingredient recipes and see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "048849307c7233eb2bea8ea4f289f49e02a517b8"
   },
   "outputs": [],
   "source": [
    "train['num_ingredients'] = train['ingredients'].apply(lambda x: len(x))\n",
    "train = train[train['num_ingredients'] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7cf37767200798f40405dcfecf862f9eabe2a89b"
   },
   "source": [
    "## 3. Preprocess\n",
    "\n",
    "Currently, the preprocess is like below.\n",
    "\n",
    "- convert to lowercase\n",
    "- remove hyphen\n",
    "- remove numbers\n",
    "- remove words which consist of less than 2 characters\n",
    "- lemmatize\n",
    "\n",
    "This process can be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "11919def05f043269b6ab823636468f29ae1651f"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess(ingredients):\n",
    "    ingredients_text = ' '.join(ingredients)\n",
    "    ingredients_text = ingredients_text.lower()\n",
    "    ingredients_text = ingredients_text.replace('-', ' ')\n",
    "    words = []\n",
    "    for word in ingredients_text.split():\n",
    "        if re.findall('[0-9]', word): continue\n",
    "        if len(word) <= 2: continue\n",
    "        if '’' in word: continue\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        if len(word) > 0: words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "for ingredient, expected in [\n",
    "    ('Eggs', 'egg'),\n",
    "    ('all-purpose flour', 'all purpose flour'),\n",
    "    ('purée', 'purée'),\n",
    "    ('1% low-fat milk', 'low fat milk'),\n",
    "    ('half & half', 'half half'),\n",
    "    ('safetida (powder)', 'safetida (powder)')\n",
    "]:\n",
    "    actual = preprocess([ingredient])\n",
    "    assert actual == expected, f'\"{expected}\" is excpected but got \"{actual}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "9004a952dab3fdbbec867ba73c424b8d6da9aa99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39752/39752 [00:04<00:00, 8076.97it/s]\n",
      "100%|██████████| 9944/9944 [00:01<00:00, 6928.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>num_ingredients</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "      <td>9</td>\n",
       "      <td>romaine lettuce black olive grape tomato garli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "      <td>11</td>\n",
       "      <td>plain flour ground pepper salt tomato ground b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "      <td>12</td>\n",
       "      <td>egg pepper salt mayonaise cooking oil green ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "      <td>4</td>\n",
       "      <td>water vegetable oil wheat salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "      <td>20</td>\n",
       "      <td>black pepper shallot cornflour cayenne pepper ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients  \\\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...   \n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...   \n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...   \n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]   \n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe...   \n",
       "\n",
       "   num_ingredients                                                  x  \n",
       "0                9  romaine lettuce black olive grape tomato garli...  \n",
       "1               11  plain flour ground pepper salt tomato ground b...  \n",
       "2               12  egg pepper salt mayonaise cooking oil green ch...  \n",
       "3                4                     water vegetable oil wheat salt  \n",
       "4               20  black pepper shallot cornflour cayenne pepper ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['x'] = train['ingredients'].progress_apply(lambda ingredients: preprocess(ingredients))\n",
    "test['x'] = test['ingredients'].progress_apply(lambda ingredients: preprocess(ingredients))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "55996a4fb96f4e25b43e63ba32b3fc903f19d0f6"
   },
   "source": [
    "I need to tune the parameters of TfidfVectorizer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "c363f1fa47d92507bc4ea8cfee0736dc4e567769"
   },
   "outputs": [],
   "source": [
    "vectorizer = make_pipeline(\n",
    "    TfidfVectorizer(sublinear_tf=True),\n",
    "    FunctionTransformer(lambda x: x.astype('float16'), validate=False)\n",
    ")\n",
    "\n",
    "x_train = vectorizer.fit_transform(train['x'].values)\n",
    "x_train.sort_indices()\n",
    "x_test = vectorizer.transform(test['x'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "751a78d993338727709d7d3f763b87fcb87a6131"
   },
   "source": [
    "Encode cuisines to numeric values using LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "6d98153f94dbe4129d51040062ce3127ae6aeef5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazilian': 0,\n",
       " 'british': 1,\n",
       " 'cajun_creole': 2,\n",
       " 'chinese': 3,\n",
       " 'filipino': 4,\n",
       " 'french': 5,\n",
       " 'greek': 6,\n",
       " 'indian': 7,\n",
       " 'irish': 8,\n",
       " 'italian': 9,\n",
       " 'jamaican': 10,\n",
       " 'japanese': 11,\n",
       " 'korean': 12,\n",
       " 'mexican': 13,\n",
       " 'moroccan': 14,\n",
       " 'russian': 15,\n",
       " 'southern_us': 16,\n",
       " 'spanish': 17,\n",
       " 'thai': 18,\n",
       " 'vietnamese': 19}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train['cuisine'].values)\n",
    "dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x, xt, y, yt = train_test_split(x_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "032ff9e67221ccceeff64075b56d3b707558d1c1"
   },
   "source": [
    "## 4. Create model\n",
    "\n",
    "I've tried LogisticRegression, GaussianProcessClassifier, GradientBoostingClassifier, MLPClassifier, LGBMClassifier, SGDClassifier, Keras but SVC works better so far.\n",
    "\n",
    "I need to take a look at models and the parameters more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "356b6fc5fff859f83e8a1e351d762ff87f31b935"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "estimator = SVC(C=50,\n",
    "                kernel='rbf',\n",
    "                degree=3,\n",
    "                gamma=1.57,\n",
    "                coef0=1,\n",
    "                shrinking = True,\n",
    "                probability = False,\n",
    "                max_iter=-1,\n",
    "                decision_function_shape=None,\n",
    "                random_state = None,\n",
    "                cache_size=500)\n",
    "# )\n",
    "# classifier = ExtraTreesClassifier(n_estimators=500)\n",
    "classifier = OneVsRestClassifier(estimator, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cfbec2ac72ebecf900ffcf6524562785422aa581"
   },
   "source": [
    "## 5. Train model\n",
    "\n",
    "If I become to be confident in the model, I train it with the whole train data for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "02d2775c4878fd70a6345c1605ea7e3abe9cb9cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 271 ms, total: 1.33 s\n",
      "Wall time: 12min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SVC(C=50, cache_size=500, class_weight=None, coef0=1,\n",
       "  decision_function_shape=None, degree=3, gamma=1.57, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=-1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "695d33e429e0ab905e2609752a5e3e29ce09d458"
   },
   "source": [
    "## 6. Check predicted values\n",
    "\n",
    "Check if the model fitted enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "47de4da03da0f0b817e99095dcb7be8974e531ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score on train data: 0.9996478164620648\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>brazilian</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>british</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>804.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cajun_creole</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1546.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chinese</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2673.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filipino</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>french</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greek</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indian</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>irish</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>667.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>italian</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7837.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jamaican</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>526.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>japanese</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1420.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>korean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexican</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moroccan</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>russian</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>489.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>southern_us</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thai</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1536.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vietnamese</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>824.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg / total</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39752.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              f1-score  precision  recall  support\n",
       " brazilian         1.0        1.0     1.0    467.0\n",
       " british           1.0        1.0     1.0    804.0\n",
       "cajun_creole       1.0        1.0     1.0   1546.0\n",
       " chinese           1.0        1.0     1.0   2673.0\n",
       "filipino           1.0        1.0     1.0    755.0\n",
       "french             1.0        1.0     1.0   2644.0\n",
       " greek             1.0        1.0     1.0   1174.0\n",
       "indian             1.0        1.0     1.0   2997.0\n",
       " irish             1.0        1.0     1.0    667.0\n",
       " italian           1.0        1.0     1.0   7837.0\n",
       "jamaican           1.0        1.0     1.0    526.0\n",
       "japanese           1.0        1.0     1.0   1420.0\n",
       "korean             1.0        1.0     1.0    830.0\n",
       " mexican           1.0        1.0     1.0   6436.0\n",
       "moroccan           1.0        1.0     1.0    821.0\n",
       " russian           1.0        1.0     1.0    489.0\n",
       " southern_us       1.0        1.0     1.0   4319.0\n",
       " spanish           1.0        1.0     1.0    987.0\n",
       "thai               1.0        1.0     1.0   1536.0\n",
       "vietnamese         1.0        1.0     1.0    824.0\n",
       " avg / total       1.0        1.0     1.0  39752.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = label_encoder.inverse_transform(classifier.predict(x_train))\n",
    "y_true = label_encoder.inverse_transform(y_train)\n",
    "\n",
    "print(f'accuracy score on train data: {accuracy_score(y_true, y_pred)}')\n",
    "\n",
    "def report2dict(cr):\n",
    "    rows = []\n",
    "    for row in cr.split(\"\\n\"):\n",
    "        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n",
    "        if len(parsed_row) > 0: rows.append(parsed_row)\n",
    "    measures = rows[0]\n",
    "    classes = defaultdict(dict)\n",
    "    for row in rows[1:]:\n",
    "        class_label = row[0]\n",
    "        for j, m in enumerate(measures):\n",
    "            classes[class_label][m.strip()] = float(row[j + 1].strip())\n",
    "    return classes\n",
    "report = classification_report(y_true, y_pred)\n",
    "pd.DataFrame(report2dict(report)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16513d3d20bffed6319671539f261e23eda0f245"
   },
   "source": [
    "## 6. Make submission\n",
    "\n",
    "It seems to be working well. Let's make a submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "3bdf327cf64fac66819ec6a2bc1ce0c549aed73f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18009</td>\n",
       "      <td>irish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28583</td>\n",
       "      <td>southern_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41580</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29752</td>\n",
       "      <td>cajun_creole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35687</td>\n",
       "      <td>italian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       cuisine\n",
       "0  18009         irish\n",
       "1  28583   southern_us\n",
       "2  41580       italian\n",
       "3  29752  cajun_creole\n",
       "4  35687       italian"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = label_encoder.inverse_transform(classifier.predict(x_test))\n",
    "test['cuisine'] = y_pred\n",
    "test[['id', 'cuisine']].to_csv('submission_svc(50，1.57).csv', index=False)\n",
    "test[['id', 'cuisine']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c0cfa299375fdd5bb6e785d82198c8dd9e4d696"
   },
   "source": [
    "# DNN solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31801 samples, validate on 7951 samples\n",
      "Epoch 1/30\n",
      "31801/31801 [==============================] - 2s 67us/step - loss: 1.8563 - acc: 0.4633 - val_loss: 1.1459 - val_acc: 0.6619\n",
      "Epoch 2/30\n",
      "31801/31801 [==============================] - 2s 59us/step - loss: 1.1729 - acc: 0.6605 - val_loss: 0.9905 - val_acc: 0.7044\n",
      "Epoch 3/30\n",
      "31801/31801 [==============================] - 2s 61us/step - loss: 1.0021 - acc: 0.7075 - val_loss: 0.9196 - val_acc: 0.7231\n",
      "Epoch 4/30\n",
      "31801/31801 [==============================] - 2s 62us/step - loss: 0.8956 - acc: 0.7385 - val_loss: 0.8874 - val_acc: 0.7324\n",
      "Epoch 5/30\n",
      "31801/31801 [==============================] - 2s 62us/step - loss: 0.8049 - acc: 0.7645 - val_loss: 0.8788 - val_acc: 0.7491\n",
      "Epoch 6/30\n",
      "31801/31801 [==============================] - 2s 62us/step - loss: 0.7320 - acc: 0.7882 - val_loss: 0.8633 - val_acc: 0.7555\n",
      "Epoch 7/30\n",
      "31801/31801 [==============================] - 2s 63us/step - loss: 0.6668 - acc: 0.8078 - val_loss: 0.8645 - val_acc: 0.7618\n",
      "Epoch 8/30\n",
      "31801/31801 [==============================] - 2s 63us/step - loss: 0.6120 - acc: 0.8257 - val_loss: 0.8824 - val_acc: 0.7632\n",
      "Epoch 9/30\n",
      "31801/31801 [==============================] - 2s 63us/step - loss: 0.5508 - acc: 0.8434 - val_loss: 0.9142 - val_acc: 0.7651\n",
      "Epoch 10/30\n",
      "31801/31801 [==============================] - 2s 62us/step - loss: 0.5028 - acc: 0.8555 - val_loss: 0.9219 - val_acc: 0.7675\n",
      "Epoch 11/30\n",
      "31801/31801 [==============================] - 2s 63us/step - loss: 0.4522 - acc: 0.8714 - val_loss: 0.9741 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 12/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.3896 - acc: 0.8896 - val_loss: 1.0160 - val_acc: 0.7644\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 13/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.3551 - acc: 0.9009 - val_loss: 1.0413 - val_acc: 0.7676\n",
      "Epoch 14/30\n",
      "31801/31801 [==============================] - 2s 63us/step - loss: 0.3380 - acc: 0.9047 - val_loss: 1.0605 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 15/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.3235 - acc: 0.9079 - val_loss: 1.0740 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 16/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.3105 - acc: 0.9126 - val_loss: 1.0822 - val_acc: 0.7649\n",
      "Epoch 17/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.3059 - acc: 0.9152 - val_loss: 1.0969 - val_acc: 0.7641\n",
      "Epoch 18/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.3032 - acc: 0.9185 - val_loss: 1.1014 - val_acc: 0.7652\n",
      "Epoch 19/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.2985 - acc: 0.9175 - val_loss: 1.1099 - val_acc: 0.7652\n",
      "Epoch 20/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.2941 - acc: 0.9174 - val_loss: 1.1201 - val_acc: 0.7643\n",
      "Epoch 21/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.2883 - acc: 0.9183 - val_loss: 1.1267 - val_acc: 0.7649\n",
      "Epoch 22/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.2822 - acc: 0.9214 - val_loss: 1.1338 - val_acc: 0.7641\n",
      "Epoch 23/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.2793 - acc: 0.9223 - val_loss: 1.1431 - val_acc: 0.7647\n",
      "Epoch 24/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.2735 - acc: 0.9243 - val_loss: 1.1555 - val_acc: 0.7624\n",
      "Epoch 25/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.2732 - acc: 0.9225 - val_loss: 1.1639 - val_acc: 0.7638\n",
      "Epoch 26/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.2674 - acc: 0.9255 - val_loss: 1.1719 - val_acc: 0.7618\n",
      "Epoch 27/30\n",
      "31801/31801 [==============================] - 2s 59us/step - loss: 0.2661 - acc: 0.9248 - val_loss: 1.1766 - val_acc: 0.7622\n",
      "Epoch 28/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.2619 - acc: 0.9276 - val_loss: 1.1887 - val_acc: 0.7639\n",
      "Epoch 29/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.2590 - acc: 0.9291 - val_loss: 1.1950 - val_acc: 0.7624\n",
      "Epoch 30/30\n",
      "31801/31801 [==============================] - 2s 64us/step - loss: 0.2549 - acc: 0.9292 - val_loss: 1.2012 - val_acc: 0.7633\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, GlobalAveragePooling1D, BatchNormalization, ActivityRegularization\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "random_seed = 47\n",
    "Y = to_categorical(y_train, num_classes=20)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, Y, test_size = 0.2, random_state=random_seed)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(2762,)))\n",
    "# model.add(ActivityRegularization(l1=0.0, l2=0.2))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "# model.add(ActivityRegularization(l1=0.0, l2=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(20, activation = \"softmax\"))\n",
    "\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=1e-04)\n",
    "optimizer1 = Adam(lr=0.001, decay=1e-4)\n",
    "optimizer2 = SGD(lr=0.1, decay=1e-4)\n",
    "model.compile(optimizer = optimizer1 , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=0.01, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.0001)\n",
    "earlystopping = EarlyStopping(monitor='val_acc', patience=0.001, verbose=1, mode='auto')\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                      epochs=30,\n",
    "                      batch_size=128,\n",
    "                      validation_data=(X_val,Y_val),\n",
    "                      verbose=1,callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame()\n",
    "y_predicted = model.predict(x_test)\n",
    "results = np.argmax(y_predicted,axis = 1)\n",
    "\n",
    "y_pred = label_encoder.inverse_transform(results)\n",
    "test['cuisine'] = y_pred\n",
    "test[['id', 'cuisine']].to_csv('submission_dnn.csv', index=False)\n",
    "test[['id', 'cuisine']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
