{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('labels.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "breed = set(df['breed'])\n",
    "n_class = len(breed)\n",
    "class_to_num = dict(zip(breed, range(n_class)))\n",
    "num_to_class = dict(zip(range(n_class), breed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [01:05<00:00, 156.00it/s]\n"
     ]
    }
   ],
   "source": [
    "width = 299\n",
    "X = np.zeros((n, width, width, 3), dtype=np.uint8)\n",
    "y = np.zeros((n, n_class), dtype=np.uint8)\n",
    "for i in tqdm(range(n)):\n",
    "    X[i] = cv2.resize(cv2.imread('train/%s.jpg' % df['id'][i]), (width, width))\n",
    "    y[i][class_to_num[df['breed'][i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.applications import *\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import *\n",
    "from keras.applications.inception_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(MODEL, data=X):\n",
    "    cnn_model = MODEL(include_top=False, input_shape=(width, width, 3), weights='imagenet')\n",
    "    \n",
    "    inputs = Input((width, width, 3))\n",
    "    x = inputs\n",
    "    x = Lambda(preprocess_input, name='preprocessing')(x)\n",
    "    x = cnn_model(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    cnn_model = Model(inputs, x)\n",
    "\n",
    "    features = cnn_model.predict(data, batch_size=64, verbose=1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10222/10222 [==============================] - ETA: 10:5 - ETA: 6:0 - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 59 - ETA: 58 - ETA: 57 - ETA: 57 - ETA: 56 - ETA: 55 - ETA: 55 - ETA: 54 - ETA: 53 - ETA: 53 - ETA: 52 - ETA: 51 - ETA: 51 - ETA: 50 - ETA: 50 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 47 - ETA: 46 - ETA: 45 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 35 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 78s 8ms/step\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - ETA: 2:02:0 - ETA: 54:21  - ETA: 40:3 - ETA: 35:0 - ETA: 31:3 - ETA: 27:2 - ETA: 25:1 - ETA: 22:4 - ETA: 21:3 - ETA: 17:2 - ETA: 16:5 - ETA: 14:0 - ETA: 14:0 - ETA: 12:3 - ETA: 11:5 - ETA: 10:3 - ETA: 10:2 - ETA: 9:0 - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 57s - ETA: 54 - ETA: 52 - ETA: 52 - ETA: 50 - ETA: 47 - ETA: 45 - ETA: 44 - ETA: 43 - ETA: 41 - ETA: 39 - ETA: 37 - ETA: 36 - ETA: 34 - ETA: 33 - ETA: 32 - ETA: 31 - ETA: 30 - ETA: 31 - ETA: 30 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 16s 0us/step\n",
      "10222/10222 [==============================] - ETA: 7: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 59 - ETA: 58 - ETA: 57 - ETA: 56 - ETA: 55 - ETA: 55 - ETA: 54 - ETA: 53 - ETA: 52 - ETA: 52 - ETA: 51 - ETA: 50 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 46 - ETA: 45 - ETA: 45 - ETA: 44 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 37 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 120s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "inception_features = get_features(InceptionV3, X)\n",
    "xception_features = get_features(Xception, X)\n",
    "features = np.concatenate([inception_features, xception_features], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8177 samples, validate on 2045 samples\n",
      "Epoch 1/10\n",
      "8177/8177 [==============================] - ETA: 1:34 - loss: 4.9193 - acc: 0.007 - ETA: 10s - loss: 4.6153 - acc: 0.082 - ETA: 6s - loss: 4.3157 - acc: 0.1543 - ETA: 3s - loss: 3.7176 - acc: 0.277 - ETA: 2s - loss: 3.2752 - acc: 0.360 - ETA: 1s - loss: 2.9642 - acc: 0.416 - ETA: 1s - loss: 2.7105 - acc: 0.458 - ETA: 0s - loss: 2.4644 - acc: 0.499 - ETA: 0s - loss: 2.2933 - acc: 0.530 - ETA: 0s - loss: 2.1718 - acc: 0.552 - 2s 277us/step - loss: 2.0266 - acc: 0.5782 - val_loss: 0.7636 - val_acc: 0.8034\n",
      "Epoch 2/10\n",
      "8177/8177 [==============================] - ETA: 0s - loss: 0.7159 - acc: 0.820 - ETA: 0s - loss: 0.6509 - acc: 0.847 - ETA: 0s - loss: 0.6443 - acc: 0.841 - ETA: 0s - loss: 0.6324 - acc: 0.841 - ETA: 0s - loss: 0.6131 - acc: 0.845 - ETA: 0s - loss: 0.6137 - acc: 0.844 - ETA: 0s - loss: 0.6063 - acc: 0.843 - ETA: 0s - loss: 0.5993 - acc: 0.844 - ETA: 0s - loss: 0.5952 - acc: 0.844 - 1s 75us/step - loss: 0.5964 - acc: 0.8425 - val_loss: 0.5842 - val_acc: 0.8264\n",
      "Epoch 3/10\n",
      "8177/8177 [==============================] - ETA: 0s - loss: 0.4659 - acc: 0.882 - ETA: 0s - loss: 0.4511 - acc: 0.886 - ETA: 0s - loss: 0.4493 - acc: 0.883 - ETA: 0s - loss: 0.4572 - acc: 0.876 - ETA: 0s - loss: 0.4569 - acc: 0.876 - ETA: 0s - loss: 0.4510 - acc: 0.875 - ETA: 0s - loss: 0.4448 - acc: 0.877 - ETA: 0s - loss: 0.4426 - acc: 0.876 - ETA: 0s - loss: 0.4459 - acc: 0.874 - ETA: 0s - loss: 0.4476 - acc: 0.873 - 1s 74us/step - loss: 0.4453 - acc: 0.8738 - val_loss: 0.5367 - val_acc: 0.8430\n",
      "Epoch 4/10\n",
      "8177/8177 [==============================] - ETA: 0s - loss: 0.4701 - acc: 0.828 - ETA: 0s - loss: 0.4115 - acc: 0.881 - ETA: 0s - loss: 0.3859 - acc: 0.888 - ETA: 0s - loss: 0.3705 - acc: 0.898 - ETA: 0s - loss: 0.3621 - acc: 0.898 - ETA: 0s - loss: 0.3604 - acc: 0.899 - ETA: 0s - loss: 0.3608 - acc: 0.897 - ETA: 0s - loss: 0.3621 - acc: 0.897 - 1s 65us/step - loss: 0.3663 - acc: 0.8945 - val_loss: 0.5152 - val_acc: 0.8357\n",
      "Epoch 5/10\n",
      "8177/8177 [==============================] - ETA: 0s - loss: 0.2379 - acc: 0.945 - ETA: 0s - loss: 0.3137 - acc: 0.912 - ETA: 0s - loss: 0.2975 - acc: 0.916 - ETA: 0s - loss: 0.2923 - acc: 0.921 - ETA: 0s - loss: 0.2935 - acc: 0.919 - ETA: 0s - loss: 0.3007 - acc: 0.916 - ETA: 0s - loss: 0.3028 - acc: 0.915 - ETA: 0s - loss: 0.2979 - acc: 0.917 - ETA: 0s - loss: 0.2997 - acc: 0.916 - ETA: 0s - loss: 0.3033 - acc: 0.915 - 1s 74us/step - loss: 0.3036 - acc: 0.9153 - val_loss: 0.5003 - val_acc: 0.8401\n",
      "Epoch 6/10\n",
      "8177/8177 [==============================] - ETA: 0s - loss: 0.2383 - acc: 0.937 - ETA: 0s - loss: 0.2719 - acc: 0.929 - ETA: 0s - loss: 0.2627 - acc: 0.934 - ETA: 0s - loss: 0.2635 - acc: 0.932 - ETA: 0s - loss: 0.2701 - acc: 0.930 - ETA: 0s - loss: 0.2654 - acc: 0.931 - ETA: 0s - loss: 0.2587 - acc: 0.932 - ETA: 0s - loss: 0.2617 - acc: 0.930 - ETA: 0s - loss: 0.2599 - acc: 0.931 - 1s 67us/step - loss: 0.2610 - acc: 0.9307 - val_loss: 0.4844 - val_acc: 0.8523\n",
      "Epoch 7/10\n",
      "8177/8177 [==============================] - ETA: 0s - loss: 0.2436 - acc: 0.937 - ETA: 0s - loss: 0.2372 - acc: 0.934 - ETA: 0s - loss: 0.2424 - acc: 0.931 - ETA: 0s - loss: 0.2415 - acc: 0.937 - ETA: 0s - loss: 0.2412 - acc: 0.935 - ETA: 0s - loss: 0.2347 - acc: 0.937 - ETA: 0s - loss: 0.2349 - acc: 0.936 - ETA: 0s - loss: 0.2326 - acc: 0.937 - ETA: 0s - loss: 0.2318 - acc: 0.937 - 1s 79us/step - loss: 0.2323 - acc: 0.9373 - val_loss: 0.4865 - val_acc: 0.8465\n",
      "Epoch 8/10\n",
      "8177/8177 [==============================] - ETA: 0s - loss: 0.1887 - acc: 0.929 - ETA: 0s - loss: 0.1855 - acc: 0.945 - ETA: 0s - loss: 0.1846 - acc: 0.951 - ETA: 0s - loss: 0.1897 - acc: 0.949 - ETA: 0s - loss: 0.1933 - acc: 0.949 - ETA: 0s - loss: 0.1963 - acc: 0.948 - ETA: 0s - loss: 0.1974 - acc: 0.950 - ETA: 0s - loss: 0.1972 - acc: 0.948 - ETA: 0s - loss: 0.2003 - acc: 0.948 - ETA: 0s - loss: 0.2040 - acc: 0.946 - ETA: 0s - loss: 0.2030 - acc: 0.946 - 1s 83us/step - loss: 0.2010 - acc: 0.9480 - val_loss: 0.4905 - val_acc: 0.8430\n",
      "Epoch 9/10\n",
      "8177/8177 [==============================] - ETA: 0s - loss: 0.1564 - acc: 0.976 - ETA: 0s - loss: 0.1763 - acc: 0.958 - ETA: 0s - loss: 0.1786 - acc: 0.955 - ETA: 0s - loss: 0.1780 - acc: 0.955 - ETA: 0s - loss: 0.1778 - acc: 0.955 - ETA: 0s - loss: 0.1834 - acc: 0.952 - ETA: 0s - loss: 0.1846 - acc: 0.952 - ETA: 0s - loss: 0.1832 - acc: 0.952 - ETA: 0s - loss: 0.1819 - acc: 0.952 - ETA: 0s - loss: 0.1827 - acc: 0.952 - 1s 76us/step - loss: 0.1823 - acc: 0.9528 - val_loss: 0.4811 - val_acc: 0.8479\n",
      "Epoch 10/10\n",
      "8177/8177 [==============================] - ETA: 0s - loss: 0.1741 - acc: 0.960 - ETA: 0s - loss: 0.1515 - acc: 0.964 - ETA: 0s - loss: 0.1485 - acc: 0.964 - ETA: 0s - loss: 0.1509 - acc: 0.962 - ETA: 0s - loss: 0.1533 - acc: 0.961 - ETA: 0s - loss: 0.1545 - acc: 0.960 - ETA: 0s - loss: 0.1572 - acc: 0.959 - ETA: 0s - loss: 0.1568 - acc: 0.959 - ETA: 0s - loss: 0.1577 - acc: 0.960 - ETA: 0s - loss: 0.1585 - acc: 0.960 - 1s 78us/step - loss: 0.1582 - acc: 0.9605 - val_loss: 0.4792 - val_acc: 0.8440\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(features.shape[1:])\n",
    "x = inputs\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(n_class, activation='softmax')(x)\n",
    "model = Model(inputs, x)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "h = model.fit(features, y, batch_size=128, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10357/10357 [01:23<00:00, 123.35it/s]\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('sample_submission.csv')\n",
    "n_test = len(df2)\n",
    "X_test = np.zeros((n_test, width, width, 3), dtype=np.uint8)\n",
    "for i in tqdm(range(n_test)):\n",
    "    X_test[i] = cv2.resize(cv2.imread('test/%s.jpg' % df2['id'][i]), (width, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357/10357 [==============================] - ETA: 11:1 - ETA: 6:1 - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 58 - ETA: 58 - ETA: 57 - ETA: 56 - ETA: 56 - ETA: 55 - ETA: 54 - ETA: 54 - ETA: 53 - ETA: 52 - ETA: 52 - ETA: 51 - ETA: 51 - ETA: 50 - ETA: 49 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 79s 8ms/step\n",
      "10357/10357 [==============================] - ETA: 12:2 - ETA: 7:0 - ETA: 5: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 59 - ETA: 58 - ETA: 57 - ETA: 56 - ETA: 55 - ETA: 54 - ETA: 54 - ETA: 53 - ETA: 52 - ETA: 51 - ETA: 50 - ETA: 50 - ETA: 49 - ETA: 48 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 44 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 40 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 37 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 33 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 29 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 22 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 18 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 11 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 126s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "inception_features = get_features(InceptionV3, X_test)\n",
    "xception_features = get_features(Xception, X_test)\n",
    "features_test = np.concatenate([inception_features, xception_features], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(features_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in breed:\n",
    "    df2[b] = y_pred[:,class_to_num[b]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('pred.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
