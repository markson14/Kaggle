{
  "cells": [
    {
      "metadata": {
        "_uuid": "ee62213bb0b14fde548560f842f175a29e62911c"
      },
      "cell_type": "markdown",
      "source": "**Importing required Packages**"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import math\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nimport os\nfrom scipy import ndimage\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nnp.random.seed(1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f9e405f698091cb1e0efc005f5768a639041dd2b"
      },
      "cell_type": "markdown",
      "source": "**Getting familiar with our dataset**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "083a759eebc2bd5509e52ba83b75108f2722be50",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# First Source of data\ntrain_data_1 = pd.read_csv(\"../input/digit-recognizer/train.csv\")\n# Second Source of data\ntrain_data_2 = pd.read_csv(\"../input/mnist-in-csv/mnist_train.csv\")\n# Third Source of data\ntrain_data_3 = pd.read_csv(\"../input/mnist-in-csv/mnist_test.csv\")\n# Combine the data\ntrain_data = pd.DataFrame(np.row_stack((train_data_1,train_data_2, train_data_3)))\n#train_data = pd.DataFrame(np.row_stack((train_data_2, train_data_3)))\n# Shuffle the data\ntrain_data = train_data.sample(frac=1).reset_index(drop=True)\n# Load test data\nX_test = pd.read_csv(\"../input/digit-recognizer/test.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "82e339f0b4a8ea769e4c4db5c8010f539bc88bb7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Separate into X_train and y_train(target varible)\ny_train = train_data[0]\nX_train = train_data.drop(0,axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "36fd18fd1b2f8eb509c0fa514631a0c2778af185",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Shape of X_train\nprint(\"Shape of X_train :\",X_train.shape)\nprint(\"Shape of X_test  :\",X_test.shape)\n# Shape of y_train\nprint(\"Shape of y_train :\",y_train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "478d700ee0b3b0feb91b84136b8da0e53c478c68",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Converting to float values\nX_train = X_train.values.astype('float32')\nX_test = X_test.values.astype('float32')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef21ca2c3365d4ab11e4d52fe8152c4e740bd2cc",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Each image is a 28x28 pixel image so resulting into 784 pixel values\n# Reshape X_train : m * height * width * 1\n# One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions, i.e. m.\nX_train = X_train.reshape([-1, 28, 28,1])\nX_test = X_test.reshape([-1, 28, 28,1])\n# Shape of X_train\nprint(\"Shape of X_train now :\",X_train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb683e3b395fca7747644cbf5204f7c6998f50e5",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Example of a picture from dataset\nindex = 10\nplt.imshow(X_train[index,:,:,0],cmap = \"gray\")\nplt.title(\"y = \"+str(y_train[index]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b95dcded71a7a89e227be72bd7e6b640949b28d4"
      },
      "cell_type": "markdown",
      "source": "**Exploratory Data Analysis**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1a893ccbb4f4ba3b79cd3c32fa86b0b7bfadcb0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Countplot for labels\nsns.countplot(train_data[0])\nprint(\"Dataset is pretty balanced!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3a36290eb7efe99321083f0724fcdd97d6fe4710"
      },
      "cell_type": "markdown",
      "source": "**CNN Architecture we're going to use**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a63bef51ec494d31386eca2e1718f68b641d0c6",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from IPython.display import Image\nImage(filename = \"../input/cnn-architecture/Untitled.png\", width=1900, unconfined=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "a8b98e02506b64bd79e5f9271e4bb6be99756bd5"
      },
      "cell_type": "code",
      "source": "# Normalise input\nX_train = X_train/255.0\nX_test = X_test/255.0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "57fa3374755cb5954e8c761f1361e9ec1710eaea",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "**Defining Helper Functions to use later**"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "131e37decc1c30c26c1bb94dc4982f6dd852e301"
      },
      "cell_type": "code",
      "source": "# Helper function for one-hot encoding of target variable\n# one_hot encoder\n# 0 -> [1 0 0 0 0 0 0 0 0 0]\n# 1 -> [0 1 0 0 0 0 0 0 0 0]\n# ...\ndef onehot_encoder(label, k):\n    \"\"\"\n    Arguments:\n    labels -- labels of images(target variable)\n    k      -- number of classes\n    Returns:\n    label_one_hot -- numpy matrix with one-hot encoded labels of shape (m,k)\n    \"\"\"\n    m = label.shape[0] # number of training examples\n    label_one_hot = np.zeros((m, k))\n    for i in range(m):\n        label_one_hot[i][int(label[i])] = 1\n    return label_one_hot\n\n# Helper function for creating placeholders\ndef create_placeholders(n_H0, n_W0, n_C0, n_y):\n    \"\"\"\n    Arguments:\n    n_H0 -- scalar, height of an input image\n    n_W0 -- scalar, width of an input image\n    n_C0 -- scalar, number of channels of the input\n    n_y -- scalar, number of classes  \n    Returns:\n    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n    is_training -- boolean value(True/False) to tell if we are working on train set or test/dev set. Useful for BatchNorm layer\n    \"\"\"\n    X = tf.placeholder(tf.float32,[None,n_H0,n_W0,n_C0])\n    Y = tf.placeholder(tf.float32,[None,n_y])\n    is_training = tf.placeholder(tf.bool)\n    return X, Y, is_training\n\n# Helper function for creating random mini batches\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Arguments:\n    X -- input data, of shape (number of examples,28,28,1)\n    Y -- true \"label\" vector of shape (number of examples, number of classes)\n    mini_batch_size -- size of the mini-batches, integer\n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    np.random.seed(seed)            # for reproducibility\n    m = X.shape[0]                  # number of training examples\n    mini_batches = []\n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[permutation,:,:,:]\n    shuffled_Y = Y[permutation,:]\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[k * mini_batch_size: (k + 1) * mini_batch_size, :, :, :]\n        mini_batch_Y = shuffled_Y[k * mini_batch_size: (k + 1) * mini_batch_size, :]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size: m, :, :, :]\n        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size: m, :]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    return mini_batches\n\n# Helper function for creating sequence mini batches\n# Will be used for prediction of test data\ndef sequence_mini_batches(X, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Arguments:\n    X -- input data, of shape (number of examples,28,28,1)\n    mini_batch_size -- size of the mini-batches, integer\n    Returns:\n    mini_batches -- list of synchronous mini_batch_X\n    \"\"\"\n    np.random.seed(seed)            # for reproducibility\n    m = X.shape[0]                  # number of training examples\n    mini_batches = []\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = X[k * mini_batch_size: (k + 1) * mini_batch_size, :, :, :]\n        mini_batches.append(mini_batch_X)\n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = X[num_complete_minibatches * mini_batch_size: m, :, :, :]\n        mini_batches.append(mini_batch_X)\n    return mini_batches\n\n# Helper function for initializing parameters\ndef initialize_parameters():\n    \"\"\"\n    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n                        W1 : [5, 5, 1, 32]\n                        W2 : [5, 5, 32, 32]\n                        W3 : [3, 3, 32, 64]\n                        W4 : [3, 3, 64, 64]\n    Returns:\n    parameters -- a dictionary of tensors containing W1, W2\n    \"\"\"\n    tf.set_random_seed(1)                              # for reproducibility\n    W1 = tf.get_variable('W1',[5, 5, 1, 32],initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n    W2 = tf.get_variable('W2',[5, 5, 32, 32],initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n    W3 = tf.get_variable('W3',[3, 3, 32, 64],initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n    W4 = tf.get_variable('W4',[3, 3, 64, 64],initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n    parameters = {\"W1\": W1,\n                  \"W2\": W2,\n                  \"W3\": W3,\n                  \"W4\": W4}\n    return parameters\n\n# Helper function for forward propagation\ndef forward_propagation(X,is_training, parameters):\n    \"\"\"\n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n                  the shapes are given in initialize_parameters\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    W3 = parameters['W3']\n    W4 = parameters['W4']\n    # CONV2D: stride of 1, padding 'SAME'\n    Z1 = tf.nn.conv2d(X,W1,strides = [1,1,1,1],padding = 'SAME')\n    # BATCHNORM\n    N1 = tf.layers.batch_normalization(Z1, training=is_training)    \n    # RELU\n    A1 = tf.nn.relu(N1)\n    # CONV2D: stride of 1, padding 'SAME'\n    Z2 = tf.nn.conv2d(A1,W2,strides = [1,1,1,1],padding = 'SAME')\n    # BATCHNORM\n    N2 = tf.layers.batch_normalization(Z2, training=is_training)    \n    # RELU\n    A2 = tf.nn.relu(N2)\n    # MAXPOOL: window 2x2, sride 2, padding 'VALID'\n    P1 = tf.nn.max_pool(A2,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'VALID')\n    # CONV2D: filters W2, stride 1, padding 'SAME'\n    Z3 = tf.nn.conv2d(P1,W3,strides = [1,1,1,1],padding = 'SAME')\n    # BATCHNORM\n    N3 = tf.layers.batch_normalization(Z3, training=is_training)\n    # RELU\n    A3 = tf.nn.relu(N3)\n    # CONV2D: filters W2, stride 1, padding 'SAME'\n    Z4 = tf.nn.conv2d(A3,W4,strides = [1,1,1,1],padding = 'SAME')\n    # BATCHNORM\n    N4 = tf.layers.batch_normalization(Z4, training=is_training)\n    # RELU\n    A4 = tf.nn.relu(N4)\n    # MAXPOOL: window 2x2, stride 2, padding 'VALID'\n    P2 = tf.nn.max_pool(A2,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'VALID')\n    # FLATTEN\n    P2 = tf.contrib.layers.flatten(P2)\n    # FULLY-CONNECTED with \"relu\" activation function\n    # 256 neurons in output layer.\n    Z5 = tf.contrib.layers.fully_connected(P2,num_outputs = 256,activation_fn=tf.nn.relu)\n    # FULLY-CONNECTED with non-linear activation function (do not call softmax)\n    # 10 neurons in output layer.\n    Z6 = tf.contrib.layers.fully_connected(Z5,num_outputs = 10,activation_fn=None)\n    return Z6\n\n# Helper function to compute cost\ndef compute_cost(Z6, Y):\n    \"\"\"\n    Arguments:\n    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (10, number of examples)\n    Y -- \"true\" labels vector placeholder, same shape as Z3\n    Returns:\n    cost - Tensor of the cost function\n    \"\"\"\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=Z6))\n    return cost",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "02e2add1f29e71581e0a7ceeda992cd39583c49e"
      },
      "cell_type": "markdown",
      "source": "**Defining the model using our helper functions**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11af1aace350a08047cfc0d97b73dea914e38597",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Defining the Model\ndef model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,num_epochs = 100, minibatch_size = 64, print_cost = True):\n    \"\"\"\n    Arguments:\n    X_train -- training set, of shape (None, 28, 28, 1)\n    Y_train -- test set, of shape (None, n_y = 10)\n    X_test -- training set, of shape (None, 28, 28, 1)\n    Y_test -- test set, of shape (None, n_y = 10)\n    learning_rate -- learning rate of the optimization\n    num_epochs -- number of epochs of the optimization loop\n    minibatch_size -- size of a minibatch\n    print_cost -- True to print the cost every 100 epochs\n    Returns:\n    train_accuracy -- real number, accuracy on the train set (X_train)\n    test_accuracy -- real number, testing accuracy on the test set (X_test)\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    tf.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n    seed = 3                                          # to keep results consistent (numpy seed)\n    (m, n_H0, n_W0, n_C0) = X_train.shape             \n    n_y = Y_train.shape[1]                            \n    costs = []                                        # To keep track of the cost\n    \n    # Create Placeholders of the correct shape\n    X, Y, is_training = create_placeholders(28,28,1,10)\n    # Initialize parameters\n    parameters = initialize_parameters()\n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    Z6 = forward_propagation(X,is_training,parameters)\n    # Cost function: Add cost function to tensorflow graph\n    cost = compute_cost(Z6,Y)\n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n    optimizer = tf.train.AdamOptimizer().minimize(cost)\n    # Initialize all the variables globally\n    init = tf.global_variables_initializer()\n    # Start the session to compute the tensorflow graph\n    config = tf.ConfigProto()\n    config.gpu_options.allocator_type = 'BFC'\n    with tf.Session(config = config) as sess:\n        # Run the initialization\n        sess.run(init)\n        # Do the training loop\n        for epoch in range(num_epochs):\n            minibatch_cost = 0.\n            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n                _ , temp_cost = sess.run([optimizer,cost],{X:minibatch_X,Y:minibatch_Y, is_training:True})\n                \n                minibatch_cost += temp_cost / num_minibatches #averaging the cost on 1 mini_batch\n                \n            # Print the cost every epoch\n            if print_cost == True and epoch % 5 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n            if print_cost == True and epoch % 1 == 0:\n                costs.append(minibatch_cost)\n        \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # Calculate the correct predictions\n        # computational graph\n        predict_op = tf.argmax(Z6, 1)\n        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n        \n        minibatches_train = random_mini_batches(X_train, Y_train, minibatch_size, seed=0)\n        train_accuracy_list = []\n        for minibatch_train in minibatches_train:\n            (minibatch_X_train, minibatch_Y_train) = minibatch_train\n            train_accuracy = accuracy.eval(session=sess, feed_dict={X: minibatch_X_train, Y: minibatch_Y_train, is_training:False})\n            train_accuracy_list.append(train_accuracy)\n        print(\"Train Accuracy:\", sum(train_accuracy_list) / len(train_accuracy_list))\n\n        if Y_test is not None: # While working on dev set\n            minibatches_dev  = random_mini_batches(X_test, Y_test, minibatch_size, seed=0)\n            dev_accuracy_list   = []\n            for minibatch_dev in minibatches_dev:\n                (minibatch_X_dev, minibatch_Y_dev)     = minibatch_dev\n                dev_accuracy = accuracy.eval(session=sess, feed_dict={X: minibatch_X_dev, Y: minibatch_Y_dev, is_training:False})\n                dev_accuracy_list.append(dev_accuracy)\n            print(\"Dev set Accuracy:\", sum(dev_accuracy_list) / len(dev_accuracy_list))\n            \n        else: # While working on submission\n            minibatches_test  = sequence_mini_batches(X_test, minibatch_size, seed=0)\n            y_pred = []\n            for minibatch_test in minibatches_test:\n                y_pred_minibatch = list(predict_op.eval(session=sess, feed_dict={X: minibatch_test, is_training:False}))\n                for l in range(0,len(y_pred_minibatch)):\n                    y_pred.append(y_pred_minibatch[l])\n                \n            y_pred = np.array(y_pred)\n            y_pred_df = pd.DataFrame(y_pred.reshape(-1, 1), index=np.arange(1, 1 + len(y_pred)), columns=[\"Label\"])\n            print(\"Generated predictions!\")\n            return y_pred_df,parameters\n        return parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cba82664bfe93a684e07fe1bc2d644ad99cdc0af"
      },
      "cell_type": "markdown",
      "source": "**Time to run the model**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75f39263684eb83fdaf2491337d37f24ab09a65f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# One-hot ecoding the labels\ny_train_one_hot = onehot_encoder(y_train, 10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f126c6c0ee5bc25523c219c6f6019551d9e03efb",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Splits data in train and dev set\n#X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train_one_hot, test_size = 0.1, random_state=42)\n# Run this model while testing\n#parameters = model(X_train, y_train, X_dev, y_dev, learning_rate = 0.01,num_epochs = 5, minibatch_size = 64, print_cost = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b814dfefdd4e48135f9d785f2364d3509cd3c2d2",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Run this model while submitting\ny_pred_df,parameters = model(X_train, y_train_one_hot, X_test, Y_test = None, learning_rate = 0.01,num_epochs = 35, minibatch_size = 64, print_cost = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "ce73ac6813e06133338d28b463241120c2c6243b"
      },
      "cell_type": "code",
      "source": "y_pred_df.to_csv(\"test_predict.csv\", index=True, index_label=\"ImageId\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b1b44f57762b81f693b4de718cfb06a8fbc61a9"
      },
      "cell_type": "markdown",
      "source": "   **GIVE A THUMBS UP IF YOU LIKE THIS KERNEL**"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "740bfa3aa8e2425a390bfa0e90875575b0350351"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}